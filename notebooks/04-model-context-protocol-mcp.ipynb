{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4143bb59",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/04-model-context-protocol-mcp.ipynb)\n",
    "\n",
    "# Part 4: Model Context Protocol (MCP)\n",
    "\n",
    "The Model Context Protocol (MCP) is an open standard for connecting AI assistants to external data sources and tools. It enables seamless integration between LLMs and various services, databases, and APIs through a standardized protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dabe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mcp\n",
      "  Using cached mcp-1.9.2-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: anyio>=4.5 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from mcp) (4.9.0)\n",
      "Collecting httpx-sse>=0.4 (from mcp)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from mcp) (0.28.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.2 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from mcp) (2.11.5)\n",
      "Collecting python-multipart>=0.0.9 (from mcp)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp)\n",
      "  Using cached sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting starlette>=0.27 (from mcp)\n",
      "  Using cached starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting uvicorn>=0.23.1 (from mcp)\n",
      "  Using cached uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (0.4.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from anyio>=4.5->mcp) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from anyio>=4.5->mcp) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from httpx>=0.27->mcp) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from httpx>=0.27->mcp) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp) (0.16.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.5.2->mcp)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: click>=7.0 in /Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp) (8.2.1)\n",
      "Using cached mcp-1.9.2-py3-none-any.whl (131 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.47.0-py3-none-any.whl (72 kB)\n",
      "Using cached uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: uvicorn, python-multipart, python-dotenv, httpx-sse, starlette, sse-starlette, pydantic-settings, mcp\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8/8\u001b[0m [mcp]\n",
      "\u001b[1A\u001b[2KSuccessfully installed httpx-sse-0.4.0 mcp-1.9.2 pydantic-settings-2.9.1 python-dotenv-1.1.0 python-multipart-0.0.20 sse-starlette-2.3.6 starlette-0.47.0 uvicorn-0.34.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72b3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "else:\n",
    "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY',None)\n",
    "\n",
    "# Create client with api key\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f208c",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "\n",
    "Model Context Protocol (MCP) is a revolutionary approach to extending AI capabilities. Unlike traditional function calling where you define functions locally in your code, MCP allows AI models to connect to remote servers that provide tools and resources.\n",
    "\n",
    "\n",
    "- **üîå Plug-and-Play Integration**: Connect to any MCP-compatible service instantly\n",
    "- **üåê Remote Capabilities**: Access tools and data from anywhere on the internet\n",
    "- **üîÑ Standardized Protocol**: One protocol works with all MCP servers\n",
    "- **üîí Centralized Security**: Control access and permissions at the server level\n",
    "- **üìà Scalability**: Share resources across multiple AI applications\n",
    "- **üõ†Ô∏è Rich Ecosystem**: Growing library of MCP servers for various use case\n",
    "\n",
    "## 1. Working with Stdio MCP Servers\n",
    "\n",
    "Stdio (Standard Input/Output) servers run as local processes and communicate through pipes. This is perfect for:\n",
    "- Development and testing\n",
    "- Local tools and utilities\n",
    "- Lightweight integrations\n",
    "\n",
    "\n",
    "## 1. Working with MCP Servers\n",
    "\n",
    "Let's use the DeepWiki MCP server, which provides access to Wikipedia data and search capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70ecfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/h6/7xcgpb4s0j5d9cjp5x2_d27m0000gn/T/ipykernel_51299/52228023.py\", line 30, in <module>\n",
      "  |     await run()\n",
      "  |   File \"/var/folders/h6/7xcgpb4s0j5d9cjp5x2_d27m0000gn/T/ipykernel_51299/52228023.py\", line 9, in run\n",
      "  |     async with stdio_client(server_params) as (read, write):\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py\", line 179, in stdio_client\n",
      "  |     anyio.create_task_group() as tg,\n",
      "  |     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Exception Group Traceback (most recent call last):\n",
      "    |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py\", line 185, in stdio_client\n",
      "    |     yield read_stream, write_stream\n",
      "    |   File \"/var/folders/h6/7xcgpb4s0j5d9cjp5x2_d27m0000gn/T/ipykernel_51299/52228023.py\", line 10, in run\n",
      "    |     async with ClientSession(read, write) as session:\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 223, in __aexit__\n",
      "    |     return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "    |     raise BaseExceptionGroup(\n",
      "    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "    +-+---------------- 1 ----------------\n",
      "      | Traceback (most recent call last):\n",
      "      |   File \"/var/folders/h6/7xcgpb4s0j5d9cjp5x2_d27m0000gn/T/ipykernel_51299/52228023.py\", line 16, in run\n",
      "      |     response = await client.aio.models.generate_content(\n",
      "      |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/google/genai/models.py\", line 7453, in generate_content\n",
      "      |     response = await self._generate_content(\n",
      "      |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/google/genai/models.py\", line 6447, in _generate_content\n",
      "      |     response_dict = await self._api_client.async_request(\n",
      "      |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/google/genai/_api_client.py\", line 799, in async_request\n",
      "      |     result = await self._async_request(http_request=http_request, stream=False)\n",
      "      |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/google/genai/_api_client.py\", line 743, in _async_request\n",
      "      |     await errors.APIError.raise_for_async_response(response)\n",
      "      |   File \"/Users/ferveloz/data/formaciones/gemini-2.5-ai-engineering-workshop/gemini_2.5_venv/lib/python3.12/site-packages/google/genai/errors.py\", line 131, in raise_for_async_response\n",
      "      |     raise ServerError(status_code, response_json, response)\n",
      "      | google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
      "      +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\"-y\", \"@philschmid/weather-mcp\"],  # MCP Server\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Prompt to get the weather for the current day in London.\n",
    "            prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n",
    "            # Initialize the connection between client and server\n",
    "            await session.initialize()\n",
    "            # Send request to the model with MCP function declarations\n",
    "            response = await client.aio.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],  # uses the session, will automatically call the tool\n",
    "                    # Uncomment if you **don't** want the sdk to automatically call the tool\n",
    "                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(\n",
    "                    #     disable=True\n",
    "                    # ),\n",
    "                ),\n",
    "            )\n",
    "            print(response.text)\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ce101",
   "metadata": {},
   "source": [
    "## !! Exercise: Build Your Own MCP CLI Agent !!\n",
    "\n",
    "Create an interactive command-line interface (CLI) chat agent that connects to the DeepWiki MCP server (a remote server providing access to Wikipedia-like data). The agent should allow users to ask questions about GitHub repositories, and it will use the DeepWiki server to find answers.\n",
    "\n",
    "Task:\n",
    "- Use `mcp.client.streamable_http.streamablehttp_client` to establish a connection to the remote URL (https://mcp.deepwiki.com/mcp). \n",
    "- Inside the `async with streamablehttp_client(...)` block, create an `mcp.ClientSession`.\n",
    "- Initialize the session using `await session.initialize()`.\n",
    "- Create a `genai.types.GenerateContentConfig` with `temperature=0` and pass the `session` object in the `tools` list. This configures the chat to use the MCP server.\n",
    "- Create an asynchronous chat session using `client.aio.chats.create()`, passing the `MODEL_ID` (e.g., \"gemini-2.5-flash-preview-05-20\") and the `config` you created.\n",
    "- Implement an interactive loop to chat with the model using `input()` to get the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e3088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is ready. Type 'exit' to quit.\n",
      "hi! Can you talk a bit about huggingface/transformers repository?\n",
      "Function call: id=None args={'repoName': 'huggingface/transformers'} name='read_wiki_contents'\n",
      "Function response: # Page: Overview\n",
      "\n",
      "# Overview\n",
      "\n",
      "<details>\n",
      "<summary>Relevant source files</summary>\n",
      "\n",
      "The following files were used as context for generating this wiki page:\n",
      "\n",
      "- [README.md](README.md)\n",
      "- [docs/source/en/_toctree.yml](docs/source/en/_toctree.yml)\n",
      "- [docs/source/en/index.md](docs/source/en/index.md)\n",
      "- [docs/source/en/perf_infer_gpu_one.md](docs/source/en/perf_infer_gpu_one.md)\n",
      "- [docs/source/ko/_toctree.yml](docs/source/ko/_toctree.yml)\n",
      "- [docs/source/ko/model_doc/qwen2_vl.md](docs/source/ko/model_doc/qwen2_vl.md)\n",
      "- [src/transformers/__init__.py](src/transformers/__init__.py)\n",
      "- [src/transformers/models/__init__.py](src/transformers/models/__init__.py)\n",
      "- [src/transformers/models/auto/configuration_auto.py](src/transformers/models/auto/configuration_auto.py)\n",
      "- [src/transformers/models/auto/feature_extraction_auto.py](src/transformers/models/auto/feature_extraction_auto.py)\n",
      "- [src/transformers/models/auto/image_processing_auto.py](src/transformers/models/auto/image_processing_auto.py)\n",
      "- [src/transformers/models/auto/modeling_auto.py](src/transformers/models/auto/modeling_auto.py)\n",
      "- [src/transformers/models/auto/processing_auto.py](src/transformers/models/auto/processing_auto.py)\n",
      "- [src/transformers/models/auto/tokenization_auto.py](src/transformers/models/auto/tokenization_auto.py)\n",
      "- [src/transformers/utils/dummy_pt_objects.py](src/transformers/utils/dummy_pt_objects.py)\n",
      "- [src/transformers/utils/dummy_vision_objects.py](src/transformers/utils/dummy_vision_objects.py)\n",
      "- [utils/check_repo.py](utils/check_repo.py)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "\n",
      "The Transformers library is a comprehensive machine learning framework that provides access to state-of-the-art pre-trained models, tools for inference, fine-tuning, and training across multiple modalities including text, images, audio, and multimodal content. This page provides a high-level overview of the library's architecture, core components, and primary workflows.\n",
      "\n",
      "Transformers supports multiple deep learning frameworks (PyTorch, TensorFlow, and Flax) while maintaining a consistent API across all of them. It is designed to make powerful models accessible to researchers, engineers, and developers with varying levels of machine learning expertise.\n",
      "\n",
      "Sources: [README.md:57-66](), [src/transformers/__init__.py:15-21]()\n",
      "\n",
      "## Core Architecture\n",
      "\n",
      "The Transformers library is organized around a set of central components that work together to provide a consistent interface for working with various model architectures.\n",
      "\n",
      "```mermaid\n",
      "flowchart TD\n",
      "    subgraph \"Core Components\"\n",
      "        Config[\"Configuration<br>PretrainedConfig\"]\n",
      "        Model[\"Model<br>PreTrainedModel\"]\n",
      "        InputProc[\"Input Processing<br>PreTrainedTokenizer<br>ImageProcessor<br>FeatureExtractor\"]\n",
      "        \n",
      "        Generation[\"Generation System<br>GenerationMixin\"]\n",
      "        Trainer[\"Training System<br>Trainer\"]\n",
      "        Pipeline[\"Pipeline System<br>Pipeline\"]\n",
      "        \n",
      "        Auto[\"Auto Classes<br>AutoModel, AutoTokenizer\"]\n",
      "    end\n",
      "    \n",
      "    Config --> Model\n",
      "    InputProc --> Model\n",
      "    Model --> Generation\n",
      "    Model --> Pipeline\n",
      "    \n",
      "    Auto --> Model\n",
      "    Auto --> InputProc\n",
      "    Auto --> Config\n",
      "    \n",
      "    Model --> Trainer\n",
      "```\n",
      "\n",
      "Sources: [src/transformers/__init__.py:56-278](), [src/transformers/models/auto/modeling_auto.py:16-48](), [src/transformers/models/auto/configuration_auto.py:16-35]()\n",
      "\n",
      "### Base Classes\n",
      "\n",
      "The foundation of the library consists of three main types of classes:\n",
      "\n",
      "1. **Configuration Classes** - Store model parameters and hyperparameters:\n",
      "   - `PretrainedConfig`: Base class for all model configurations\n",
      "\n",
      "2. **Model Classes** - Implement neural network architectures:\n",
      "   - `PreTrainedModel`: Base class for PyTorch models\n",
      "   - `TFPreTrainedModel`: Base class for TensorFlow models\n",
      "   - `FlaxPreTrainedModel`: Base class for Flax models\n",
      "\n",
      "3. **Input Processing Classes** - Handle text tokenization and feature extraction:\n",
      "   - `PreTrainedTokenizerBase`: Abstract base class for all tokenizers\n",
      "   - `PreTrainedTokenizer`: Base class for slow tokenizers\n",
      "   - `PreTrainedTokenizerFast`: Base class for fast tokenizers\n",
      "   - `ImageProcessingMixin`/`BaseImageProcessor`: For vision models\n",
      "   - `FeatureExtractionMixin`: For audio and multimodal models\n",
      "\n",
      "Sources: [src/transformers/__init__.py:57-106](), [src/transformers/__init__.py:186-194](), [src/transformers/__init__.py:321-322](), [src/transformers/utils/dummy_vision_objects.py:1-24]()\n",
      "\n",
      "### Auto Classes System\n",
      "\n",
      "The Auto Classes system provides a unified interface for loading models, tokenizers, and processors based on a given checkpoint name or path. This system eliminates the need to know the specific model architecture in advance.\n",
      "\n",
      "```mermaid\n",
      "flowchart TD\n",
      "    User([User]) --> AutoClass[\"Auto Class<br>from_pretrained()\"]\n",
      "    \n",
      "    AutoClass --> ConfigMapping[\"CONFIG_MAPPING_NAMES<br>Maps model type to config class\"]\n",
      "    \n",
      "    ConfigMapping --> ModelMapping[\"MODEL_MAPPING_NAMES<br>Maps model type to model class\"]\n",
      "    ConfigMapping --> TokenizerMapping[\"TOKENIZER_MAPPING_NAMES<br>Maps model type to tokenizer class\"]\n",
      "    ConfigMapping --> ProcessorMapping[\"Processor/ImageProcessor/FeatureExtractor<br>Mapping names\"]\n",
      "    \n",
      "    ModelMapping --> ModelInstance[\"Model Instance<br>(PyTorch/TF/Flax)\"]\n",
      "    TokenizerMapping --> TokenizerInstance[\"Tokenizer Instance\"]\n",
      "    ProcessorMapping --> ProcessorInstance[\"Processor Instance\"]\n",
      "```\n",
      "\n",
      "Key auto classes include:\n",
      "- `AutoConfig`: Automatically selects the appropriate configuration class\n",
      "- `AutoModel`, `AutoModelForCausalLM`, etc.: Select appropriate model classes\n",
      "- `AutoTokenizer`: Selects the appropriate tokenizer\n",
      "- `AutoProcessor`, `AutoImageProcessor`, `AutoFeatureExtractor`: Select appropriate processing classes\n",
      "\n",
      "Sources: [src/transformers/models/auto/configuration_auto.py:32-366](), [src/transformers/models/auto/modeling_auto.py:32-413](), [src/transformers/models/auto/tokenization_auto.py:47-102](), [src/transformers/models/auto/image_processing_auto.py:40-91](), [src/transformers/models/auto/feature_extraction_auto.py:40-91](), [src/transformers/models/auto/processing_auto.py:46-97]()\n",
      "\n",
      "## Primary Workflows\n",
      "\n",
      "Transformers supports three main workflows: inference, training, and model development.\n",
      "\n",
      "### Inference with Pipeline\n",
      "\n",
      "The Pipeline API provides a high-level interface for running inference with pre-trained models on various tasks. It handles model loading, input preprocessing, and output post-processing in a single unified interface.\n",
      "\n",
      "```mermaid\n",
      "sequenceDiagram\n",
      "    participant User\n",
      "    participant Pipeline\n",
      "    participant AutoClasses\n",
      "    participant Model\n",
      "    participant Tokenizer\n",
      "    participant Processor\n",
      "    \n",
      "    User->>Pipeline: pipeline(task=\"text-generation\", model=\"llama\")\n",
      "    Pipeline->>AutoClasses: Load appropriate model and processor\n",
      "    AutoClasses->>Model: from_pretrained()\n",
      "    AutoClasses->>Tokenizer: from_pretrained()\n",
      "    Pipeline->>User: Return pipeline instance\n",
      "    \n",
      "    User->>Pipeline: pipeline(\"Hello, I am\")\n",
      "    Pipeline->>Tokenizer: Preprocess input\n",
      "    Tokenizer->>Pipeline: Return tokenized input\n",
      "    Pipeline->>Model: Forward pass / generate\n",
      "    Model->>Pipeline: Return output\n",
      "    Pipeline->>Tokenizer: Postprocess output\n",
      "    Pipeline->>User: Return final result\n",
      "```\n",
      "\n",
      "The Pipeline system supports a wide range of tasks including:\n",
      "- Text: Generation, Classification, Question Answering, etc.\n",
      "- Vision: Image Classification, Object Detection, Segmentation, etc.\n",
      "- Audio: Speech Recognition, Audio Classification, etc.\n",
      "- Multimodal: Visual Question Answering, Document Question Answering, etc.\n",
      "\n",
      "Sources: [src/transformers/__init__.py:145-181](), [README.md:105-118](), [README.md:143-178]()\n",
      "\n",
      "### Training System\n",
      "\n",
      "Transformers provides the Trainer API for fine-tuning models on custom datasets. The Trainer handles the training loop, optimization, evaluation, and many advanced features like distributed training.\n",
      "\n",
      "```mermaid\n",
      "classDiagram\n",
      "    class Trainer {\n",
      "        +model: PreTrainedModel\n",
      "        +args: TrainingArguments\n",
      "        +data_collator\n",
      "        +train_dataset\n",
      "        +eval_dataset\n",
      "        +tokenizer\n",
      "        +compute_metrics\n",
      "        +train()\n",
      "        +evaluate()\n",
      "        +predict()\n",
      "    }\n",
      "    \n",
      "    class TrainingArguments {\n",
      "        +output_dir\n",
      "        +learning_rate\n",
      "        +per_device_train_batch_size\n",
      "        +num_train_epochs\n",
      "        +gradient_accumulation_steps\n",
      "    }\n",
      "    \n",
      "    class Callbacks {\n",
      "        +on_train_begin()\n",
      "        +on_step_end()\n",
      "        +on_epoch_end()\n",
      "        +on_train_end()\n",
      "    }\n",
      "    \n",
      "    Trainer --> TrainingArguments : uses\n",
      "    Trainer --> Callbacks : notifies\n",
      "```\n",
      "\n",
      "Sources: [src/transformers/__init__.py:195-213](), [src/transformers/__init__.py:464]()\n",
      "\n",
      "### Generation System\n",
      "\n",
      "For models capable of generating text (e.g., LLMs, encoder-decoder models), Transformers offers a comprehensive generation system through the `GenerationMixin` class. This system supports various decoding strategies such as greedy search, beam search, sampling, and contrastive search.\n",
      "\n",
      "```mermaid\n",
      "flowchart TD\n",
      "    Generate[\"model.generate()\"] --> Config[\"Load GenerationConfig\"]\n",
      "    Config --> Strategy[\"Select Decoding Strategy\"]\n",
      "    \n",
      "    Strategy -->|\"num_beams=1\\ndo_sample=false\"| Greedy[\"Greedy Search\"]\n",
      "    Strategy -->|\"num_beams>1\\ndo_sample=false\"| Beam[\"Beam Search\"]\n",
      "    Strategy -->|\"num_beams=1\\ndo_sample=true\"| Sampling[\"Multinomial Sampling\"]\n",
      "    \n",
      "    Greedy & Beam & Sampling --> Process[\"Process Tokens\"]\n",
      "    \n",
      "    Process --> PrepareInputs[\"Prepare Inputs\"]\n",
      "    PrepareInputs --> Forward[\"Model Forward Pass\"]\n",
      "    Forward --> Cache[\"Update KV Cache\"]\n",
      "    Cache --> ProcessLogits[\"Process Logits\"]\n",
      "    \n",
      "    ProcessLogits --> Processors[\"Apply LogitsProcessors<br>- Min Length<br>- Top-k/Top-p<br>- Temperature<br>- Repetition Penalty\"]\n",
      "    Processors --> NextToken[\"Select Next Token\"]\n",
      "    \n",
      "    NextToken --> AddToken[\"Add Token to Sequence\"]\n",
      "    AddToken --> CheckEOS[\"Check for EOS/Length\"]\n",
      "    CheckEOS -->|\"Continue\"| PrepareInputs\n",
      "    CheckEOS -->|\"Done\"| Output[\"Return Generated Sequence\"]\n",
      "```\n",
      "\n",
      "Sources: [src/transformers/__init__.py:107-114](), [src/transformers/__init__.py:380-433](), [src/transformers/utils/dummy_pt_objects.py:173-327]()\n",
      "\n",
      "## Framework Support\n",
      "\n",
      "Transformers is designed to be compatible with multiple deep learning frameworks:\n",
      "\n",
      "| Framework | Base Model Class | Support Level |\n",
      "|-----------|-----------------|---------------|\n",
      "| PyTorch   | `PreTrainedModel` | Full support for all models |\n",
      "| TensorFlow | `TFPreTrainedModel` | Most text models supported |\n",
      "| Flax (JAX) | `FlaxPreTrainedModel` | Growing support for text models |\n",
      "\n",
      "The library automatically handles imports based on which frameworks are available. If a framework is not installed, corresponding model classes are replaced with dummy objects that raise informative errors when used.\n",
      "\n",
      "Sources: [src/transformers/__init__.py:280-546](), [src/transformers/utils/dummy_pt_objects.py:1-644]()\n",
      "\n",
      "## Supported Model Types\n",
      "\n",
      "Transformers supports hundreds of model architectures across various modalities:\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    Models[\"Model Types\"] --> Text[\"Text Models<br>- BERT, RoBERTa<br>- GPT, LLaMA<br>- T5, BART<br>- 400+ architectures\"]\n",
      "    Models --> Vision[\"Vision Models<br>- ViT, DETR<br>- Swin, ConvNeXT<br>- YOLOS, SegFormer<br>- 60+ architectures\"]\n",
      "    Models --> Audio[\"Audio Models<br>- Whisper, Wav2Vec2<br>- HuBERT, SpeechT5<br>- CLAP, AST<br>- 20+ architectures\"]\n",
      "    Models --> Multimodal[\"Multimodal Models<br>- CLIP, BLIP<br>- LLaVA, FLAVA<br>- LayoutLM, VideoCLIP<br>- 40+ architectures\"]\n",
      "```\n",
      "\n",
      "The library manages these model implementations through a modular structure, with each model type having its own subdirectory under `src/transformers/models/`.\n",
      "\n",
      "Sources: [src/transformers/models/auto/modeling_auto.py:32-327](), [src/transformers/models/auto/configuration_auto.py:32-366](), [src/transformers/models/__init__.py:14-190]()\n",
      "\n",
      "## Example Workflow\n",
      "\n",
      "Here's a typical workflow for using the Transformers library:\n",
      "\n",
      "1. **Loading a model and tokenizer**:\n",
      "   ```python\n",
      "   from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "   \n",
      "   tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
      "   model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
      "   ```\n",
      "\n",
      "2. **Using the Pipeline API for inference**:\n",
      "   ```python\n",
      "   from transformers import pipeline\n",
      "   \n",
      "   generator = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\")\n",
      "   result = generator(\"The capital of France is\", max_length=30)\n",
      "   ```\n",
      "\n",
      "3. **Fine-tuning a model**:\n",
      "   ```python\n",
      "   from transformers import Trainer, TrainingArguments\n",
      "   \n",
      "   training_args = TrainingArguments(\n",
      "       output_dir=\"./results\",\n",
      "       num_train_epochs=3,\n",
      "       per_device_train_batch_size=8,\n",
      "       warmup_steps=500,\n",
      "       logging_dir=\"./logs\",\n",
      "   )\n",
      "   \n",
      "   trainer = Trainer(\n",
      "       model=model,\n",
      "       args=training_args,\n",
      "       train_dataset=train_dataset,\n",
      "       eval_dataset=eval_dataset,\n",
      "       tokenizer=tokenizer,\n",
      "   )\n",
      "   \n",
      "   trainer.train()\n",
      "   ```\n",
      "\n",
      "Sources: [README.md:105-118](), [README.md:143-178]()\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Transformers library provides a unified interface for working with hundreds of state-of-the-art models across text, vision, audio, and multimodal domains. Its modular architecture, with a clear separation between configurations, models, and processing components, combined with the Auto Classes system, makes it easy to use for both beginners and expert users. Whether you need to run inference with a pretrained model or fine-tune one for your specific task, Transformers provides the tools and flexibility to do so across multiple deep learning frameworks.\n",
      "Assistant: The `huggingface/transformers` repository is home to a comprehensive machine learning framework that offers access to state-of-the-art pre-trained models. It provides tools for inference, fine-tuning, and training across various modalities, including text, images, audio, and multimodal content.\n",
      "\n",
      "Here's a breakdown of its key aspects:\n",
      "\n",
      "**Core Architecture:**\n",
      "The library is built around several central components that provide a consistent interface for working with diverse model architectures. These include:\n",
      "*   **Configuration Classes (`PretrainedConfig`)**: Store model parameters and hyperparameters.\n",
      "*   **Model Classes (`PreTrainedModel`, `TFPreTrainedModel`, `FlaxPreTrainedModel`)**: Implement neural network architectures for PyTorch, TensorFlow, and Flax, respectively.\n",
      "*   **Input Processing Classes (`PreTrainedTokenizer`, `ImageProcessor`, `FeatureExtractor`)**: Handle tasks like text tokenization and feature extraction for different data types.\n",
      "*   **Auto Classes (`AutoModel`, `AutoTokenizer`, etc.)**: Provide a unified interface for loading models, tokenizers, and processors without needing to know the specific architecture beforehand.\n",
      "\n",
      "**Primary Workflows:**\n",
      "The Transformers library supports three main workflows:\n",
      "*   **Inference with Pipeline**: The `Pipeline` API offers a high-level interface for running inference on various tasks (e.g., text generation, classification, question answering, image classification, speech recognition). It handles preprocessing, model loading, and post-processing.\n",
      "*   **Training System (`Trainer`)**: The `Trainer` API simplifies fine-tuning models on custom datasets. It manages the training loop, optimization, evaluation, distributed training, and mixed-precision training.\n",
      "*   **Generation System (`GenerationMixin`)**: For models capable of generating text, the `GenerationMixin` class provides a comprehensive system supporting various decoding strategies like greedy search, beam search, and sampling.\n",
      "\n",
      "**Framework Support:**\n",
      "Transformers is designed to be compatible with multiple deep learning frameworks, including:\n",
      "*   PyTorch (`PreTrainedModel`)\n",
      "*   TensorFlow (`TFPreTrainedModel`)\n",
      "*   Flax (JAX) (`FlaxPreTrainedModel`)\n",
      "\n",
      "The library automatically handles imports based on available frameworks and provides dummy objects if a framework is not installed.\n",
      "\n",
      "**Supported Model Types:**\n",
      "The library supports hundreds of model architectures across various modalities:\n",
      "*   **Text Models**: BERT, RoBERTa, GPT, LLaMA, T5, BART, and many more.\n",
      "*   **Vision Models**: ViT, DETR, Swin, ConvNeXT, YOLOS, SegFormer, etc.\n",
      "*   **Audio Models**: Whisper, Wav2Vec2, HuBERT, SpeechT5, CLAP, AST, etc.\n",
      "*   **Multimodal Models**: CLIP, BLIP, LLaVA, FLAVA, LayoutLM, VideoCLIP, etc.\n",
      "\n",
      "**Example Usage:**\n",
      "A typical workflow involves loading a model and tokenizer using the `Auto` classes and then using the `pipeline` API for inference or the `Trainer` for fine-tuning:\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Trainer, TrainingArguments\n",
      "\n",
      "# 1. Loading a model and tokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
      "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
      "\n",
      "# 2. Using the Pipeline API for inference\n",
      "generator = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\")\n",
      "result = generator(\"The capital of France is\", max_length=30)\n",
      "print(result)\n",
      "\n",
      "# 3. Fine-tuning a model (simplified example)\n",
      "# training_args = TrainingArguments(\n",
      "#     output_dir=\"./results\",\n",
      "#     num_train_epochs=3,\n",
      "#     per_device_train_batch_size=8,\n",
      "#     warmup_steps=500,\n",
      "#     logging_dir=\"./logs\",\n",
      "# )\n",
      "# trainer = Trainer(\n",
      "#     model=model,\n",
      "#     args=training_args,\n",
      "#     train_dataset=train_dataset, # assuming train_dataset is defined\n",
      "#     eval_dataset=eval_dataset,   # assuming eval_dataset is defined\n",
      "#     tokenizer=tokenizer,\n",
      "# )\n",
      "# trainer.train()\n",
      "```\n",
      "Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "remote_url = \"https://mcp.deepwiki.com/mcp\"\n",
    "\n",
    "async def run():\n",
    "    # - Use `mcp.client.streamable_http.streamablehttp_client` to establish a connection to the remote URL (https://mcp.deepwiki.com/mcp). \n",
    "    async with streamablehttp_client(remote_url) as (read, write, _):\n",
    "        # - Inside the `async with streamablehttp_client(...)` block, create an `mcp.ClientSession`.\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # - Initialize the session using `await session.initialize()`.\n",
    "            await session.initialize()\n",
    "            # - Create a `genai.types.GenerateContentConfig` with `temperature=0` and pass the `session` object in the `tools` list. This configures the chat to use the MCP server.\n",
    "            config = genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],  # uses the session, will automatically call the tool\n",
    "                    # Uncomment if you **don't** want the sdk to automatically call the tool\n",
    "                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(\n",
    "                    #     disable=True\n",
    "                    # ),\n",
    "                )\n",
    "            # Send request to the model with MCP function declarations\n",
    "            print(\"Agent is ready. Type 'exit' to quit.\")\n",
    "            # - Create an asynchronous chat session using `client.aio.chats.create()`, passing the `MODEL_ID` (e.g., \"gemini-2.5-flash-preview-05-20\") and the `config` you created.\n",
    "            chat =  client.aio.chats.create(\n",
    "                model=MODEL_ID,\n",
    "                config=config,\n",
    "            )\n",
    "            # - Implement an interactive loop to chat with the model using `input()` to get the user's input.\n",
    "            while True:\n",
    "                user_input = input(\"You: \")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    print(\"Exiting chat.\")\n",
    "                    break\n",
    "                print(user_input)\n",
    "                # Append user message to history\n",
    "                response = await chat.send_message(user_input)\n",
    "                if len(response.automatic_function_calling_history) > 0:\n",
    "                    if (\n",
    "                        response.automatic_function_calling_history[0].parts[0].text\n",
    "                        == user_input\n",
    "                    ):\n",
    "                        response.automatic_function_calling_history.pop(0)\n",
    "                    for call in response.automatic_function_calling_history:\n",
    "                        if call.parts[0].function_call:\n",
    "                            print(f\"Function call: {call.parts[0].function_call}\")\n",
    "                        elif call.parts[0].function_response:\n",
    "                            print(\n",
    "                                f\"Function response: {call.parts[0].function_response.response['result'].content[0].text}\"\n",
    "                            )\n",
    "                print(f\"Assistant: {response.text}\")\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf73b5",
   "metadata": {},
   "source": [
    "## Recap & Next Steps\n",
    "\n",
    "**What You've Learned:**\n",
    "- Understanding the Model Context Protocol (MCP) and its advantages over traditional function calling\n",
    "- Connecting to remote MCP servers using both stdio and HTTP protocols\n",
    "- Building interactive chat agents that leverage MCP capabilities\n",
    "\n",
    "**Key Takeaways:**\n",
    "- MCP enables plug-and-play integration with external services and data sources\n",
    "- Remote capabilities provide access to tools and data from anywhere on the internet\n",
    "- Standardized protocols ensure compatibility across different AI applications\n",
    "- Centralized security and permissions improve enterprise deployment scenarios\n",
    "- The MCP ecosystem is rapidly growing with servers for various use cases\n",
    "\n",
    "üéâ **Congratulations!** You've completed the Gemini 2.5 AI Engineering Workshop\n",
    "\n",
    "**More Resources:**\n",
    "- [MCP with Gemini Documentation](https://ai.google.dev/gemini-api/docs/function-calling?example=weather#model_context_protocol_mcp)\n",
    "- [Function Calling Documentation](https://ai.google.dev/gemini-api/docs/function-calling?lang=python)\n",
    "- [MCP Official Specification](https://spec.modelcontextprotocol.io/)\n",
    "- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n",
    "- [MCP Server Directory](https://github.com/modelcontextprotocol/servers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_2.5_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
